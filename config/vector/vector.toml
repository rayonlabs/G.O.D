# Vector configuration for shipping training logs to remote Loki

[api]
enabled = true
address = "0.0.0.0:8686"

# -------------------------
# Sources
# -------------------------
[sources.docker_logs]
type = "docker_logs"
docker_host = "unix:///var/run/docker.sock"
# Wildcards aren't supported here; filter via transform below.
# include_containers = ["image-trainer-*", "text-trainer-*", "downloader-*", "hf-upload-*"]
include_labels = ["task_id", "hotkey", "model", "trainer_type", "expected_repo"]

# Vector's own metrics
[sources.internal_metrics]
type = "internal_metrics"

# -------------------------
# Transforms
# -------------------------
[transforms.parse_logs]
type = "remap"
inputs = ["docker_logs"]
source = '''
# Default level
.level = "INFO"

# Try to parse: "<timestamp> <level> <content>"
re = r'^(?P<ts>\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}[.,]\d{3})\s+(?P<lvl>\w+)\s+(?P<content>.*)$'
if match!(.message, re) {
  caps = parse_regex!(.message, re)
  .message = to_string(caps.content)
  .level = upcase(to_string(caps.lvl))

  ts = to_string(caps.ts)
  fmt = "%Y-%m-%d %H:%M:%S,%3f"
  if contains(ts, "T") { fmt = "%Y-%m-%dT%H:%M:%S,%3f" }
  if contains(ts, ".") { fmt = replace(fmt, ",%3f", ".%3f") }
  .timestamp = parse_timestamp(ts, fmt) ?? now()
} else if match!(.message, r'(?i)(ERROR|WARN|WARNING|INFO|DEBUG|CRITICAL)') {
  caps2 = parse_regex!(.message, r'(?i)(ERROR|WARN|WARNING|INFO|DEBUG|CRITICAL)')
  # whole match is at index 0 (integer index)
  .level = upcase(to_string(caps2[0]))
}

# Metadata
.trainer_server = get_hostname!()
.environment = get_env_var("ENVIRONMENT") ?? "production"

# Labels from container labels (explicit null-safe fallback)
if exists(.label_task_id) && .label_task_id != null { .task_id = .label_task_id } else { .task_id = "unknown" }
if exists(.label_hotkey) && .label_hotkey != null { .hotkey = .label_hotkey } else { .hotkey = "unknown" }
if exists(.label_model) && .label_model != null { .model = .label_model } else { .model = "unknown" }
if exists(.label_trainer_type) && .label_trainer_type != null { .trainer_type = .label_trainer_type } else { .trainer_type = "unknown" }

# Counter for reduce aggregation
.count = 1

# Cleanup
del(.label_task_id)
del(.label_hotkey)
del(.label_model)
del(.label_trainer_type)
del(.label_expected_repo)
'''

# Keep only trainer containers by name prefix
[transforms.keep_trainers]
type = "filter"
inputs = ["parse_logs"]
condition = '''
match!(.container_name, r'^(image-trainer-|text-trainer-|downloader-|hf-upload-)')
'''

# Filter noisy lines (make contains() infallible)
[transforms.filter_noise]
type = "filter"
inputs = ["keep_trainers"]
condition = '''
m = to_string(.message) ?? "";
!contains(m, "Downloading shards") &&
!contains(m, "Loading checkpoint shards") &&
!contains(m, "heartbeat")
'''

# Batch similar lines
[transforms.batch_logs]
type = "reduce"
inputs = ["filter_noise"]
group_by = ["task_id", "container_name", "level"]
merge_strategies.message = "concat_newline"
merge_strategies.count = "sum"
flush_period_ms = 5000

# -------------------------
# Sinks
# -------------------------
[sinks.loki]
type = "loki"
inputs = ["docker_logs"]
endpoint = "${LOKI_ENDPOINT}"
auth.strategy = "basic"
auth.user = "${LOKI_USERNAME}"
auth.password = "${LOKI_PASSWORD}"
encoding.codec = "json"
batch.timeout_secs = 10
batch.max_bytes = 1048576
request.timeout_secs = 30
request.retry_attempts = 3
request.retry_initial_backoff_secs = 1
tls.verify_certificate = false

# Loki stream labels
labels.job = "docker-training-containers"
labels.task_id = "{{ task_id }}"
labels.hotkey = "{{ hotkey }}"
labels.container_name = "{{ container_name }}"
labels.container_id = "{{ container_id }}"
labels.trainer_type = "{{ trainer_type }}"
labels.model = "{{ model }}"

# Prometheus exporter (Vector internal metrics)
[sinks.prometheus]
type = "prometheus_exporter"
inputs = ["internal_metrics"]
address = "0.0.0.0:9090"
default_namespace = "vector"
